{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ad2f9d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:50:48.923661Z",
     "iopub.status.busy": "2025-04-17T07:50:48.923443Z",
     "iopub.status.idle": "2025-04-17T07:51:08.356888Z",
     "shell.execute_reply": "2025-04-17T07:51:08.356093Z"
    },
    "papermill": {
     "duration": 19.439221,
     "end_time": "2025-04-17T07:51:08.358368",
     "exception": false,
     "start_time": "2025-04-17T07:50:48.919147",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import random\n",
    "import gc\n",
    "import time\n",
    "import cv2\n",
    "import math\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import librosa\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from scipy import signal\n",
    "import timm\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25fe8d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.365431Z",
     "iopub.status.busy": "2025-04-17T07:51:08.365194Z",
     "iopub.status.idle": "2025-04-17T07:51:08.371122Z",
     "shell.execute_reply": "2025-04-17T07:51:08.370583Z"
    },
    "papermill": {
     "duration": 0.010606,
     "end_time": "2025-04-17T07:51:08.372286",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.361680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    # Basic paths and metadata\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    train_soundscapes = '/kaggle/input/birdclef-2025/train_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_path = '/kaggle/input/bestmodel/pytorch/default/1'\n",
    "    pretrained = True\n",
    "\n",
    "    # Debug settings\n",
    "    debug = False\n",
    "    debug_start_num = 0  # Start index for debug sample subset\n",
    "    debug_num = 8        # Number of samples to process in debug mode\n",
    "\n",
    "    # Audio parameters\n",
    "    FS = 32000           # Sampling rate (Hz)\n",
    "    WINDOW_SIZE = 5      # Window size in seconds\n",
    "\n",
    "    # Mel spectrogram parameters\n",
    "    N_FFT = 1034\n",
    "    HOP_LENGTH = 64\n",
    "    N_MELS = 136\n",
    "    FMIN = 20\n",
    "    FMAX = 16000\n",
    "    TARGET_SHAPE = (256, 256)  # Final spectrogram shape (HxW)\n",
    "\n",
    "    # Model parameters\n",
    "    model_name = 'efficientnet_b0'\n",
    "    in_channels = 1\n",
    "    device = 'cpu'  # Force to run on CPU\n",
    "\n",
    "    # Inference parameters\n",
    "    batch_size = 16        # Smaller batch size for CPU usage\n",
    "    use_tta = False        # Disable test-time augmentation for speed\n",
    "    tta_count = 3          # Number of TTA repetitions\n",
    "    threshold = 0.7        # Default decision threshold\n",
    "\n",
    "    # Fold selection\n",
    "    use_specific_folds = False  # Use all available model folds by default\n",
    "    folds = [0, 1]              # Only used if use_specific_folds = True\n",
    "\n",
    "    # Prediction smoothing\n",
    "    apply_smoothing = True\n",
    "    smoothing_window = 5\n",
    "    smoothing_weights = [0.15, 0.2, 0.3, 0.2, 0.15]  # Symmetric smoothing kernel\n",
    "\n",
    "    # Audio preprocessing options\n",
    "    apply_noise_reduction = True\n",
    "    apply_normalization = True\n",
    "    noise_reduction_strength = 0.1\n",
    "\n",
    "    # Memory management\n",
    "    clear_cache_frequency = 5  # Clear memory every N files to prevent memory leaks\n",
    "\n",
    "    # SpecAugment options\n",
    "    use_spec_augment = True\n",
    "    time_mask_param = 30       # Max width of time mask\n",
    "    freq_mask_param = 20       # Max height of frequency mask\n",
    "    time_mask_count = 1        # Number of time masks to apply\n",
    "    freq_mask_count = 1        # Number of frequency masks to apply\n",
    "\n",
    "    # Spectrogram contrast enhancement\n",
    "    apply_spec_contrast = True\n",
    "    contrast_factor = 0.15     # Intensity of contrast boost\n",
    "\n",
    "    # Threshold adjustment\n",
    "    class_thresholds = None    # Optional per-class thresholding (set at runtime)\n",
    "\n",
    "    # Prediction blending (temporal smoothing with neighbors)\n",
    "    prediction_blend = [0.7, 0.3]  # 70% current, 30% neighboring predictions\n",
    "\n",
    "    # Additional smoothing from reference implementation\n",
    "    apply_secondary_smoothing = True\n",
    "    secondary_smoothing_weights = [0.2, 0.6, 0.2]  # Weights: [prev, current, next]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab44bddc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.378647Z",
     "iopub.status.busy": "2025-04-17T07:51:08.378439Z",
     "iopub.status.idle": "2025-04-17T07:51:08.401920Z",
     "shell.execute_reply": "2025-04-17T07:51:08.401210Z"
    },
    "papermill": {
     "duration": 0.027814,
     "end_time": "2025-04-17T07:51:08.403032",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.375218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading label data...\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Memory Management Function\n",
    "def clear_memory():\n",
    "    \"\"\"\n",
    "    Force garbage collection and clear PyTorch CUDA cache if available.\n",
    "    Helps reduce memory usage during large-scale processing or inference.\n",
    "    \"\"\"\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "##############################################################################\n",
    "# Load and Process Label Information\n",
    "print(\"Loading label data...\")\n",
    "\n",
    "# Extract primary label names from sample submission file (excluding 'row_id')\n",
    "primary_labels = pd.read_csv('/kaggle/input/birdclef-2025/sample_submission.csv').columns[1:].to_list()\n",
    "\n",
    "# Load taxonomy metadata which maps species labels to broader class categories\n",
    "taxonomy = pd.read_csv(CFG.taxonomy_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52f7b900",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.410196Z",
     "iopub.status.busy": "2025-04-17T07:51:08.409971Z",
     "iopub.status.idle": "2025-04-17T07:51:08.417664Z",
     "shell.execute_reply": "2025-04-17T07:51:08.416967Z"
    },
    "papermill": {
     "duration": 0.012518,
     "end_time": "2025-04-17T07:51:08.418711",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.406193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "        cfg.num_classes = len(taxonomy_df)\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg.model_name,\n",
    "            pretrained=cfg.pretrained,\n",
    "            in_chans=cfg.in_channels,\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2\n",
    "        )\n",
    "        \n",
    "        if 'efficientnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "        elif 'resnet' in cfg.model_name:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "            self.backbone.fc = nn.Identity()\n",
    "        else:\n",
    "            backbone_out = self.backbone.get_classifier().in_features\n",
    "            self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "        self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "            \n",
    "        self.feat_dim = backbone_out\n",
    "        \n",
    "        self.classifier = nn.Linear(backbone_out, cfg.num_classes)\n",
    "        \n",
    "        self.mixup_enabled = hasattr(cfg, 'mixup_alpha') and cfg.mixup_alpha > 0\n",
    "        if self.mixup_enabled:\n",
    "            self.mixup_alpha = cfg.mixup_alpha\n",
    "            \n",
    "    def forward(self, x, targets=None):\n",
    "    \n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            mixed_x, targets_a, targets_b, lam = self.mixup_data(x, targets)\n",
    "            x = mixed_x\n",
    "        else:\n",
    "            targets_a, targets_b, lam = None, None, None\n",
    "        \n",
    "        features = self.backbone(x)\n",
    "        \n",
    "        if isinstance(features, dict):\n",
    "            features = features['features']\n",
    "            \n",
    "        if len(features.shape) == 4:\n",
    "            features = self.pooling(features)\n",
    "            features = features.view(features.size(0), -1)\n",
    "        \n",
    "        logits = self.classifier(features)\n",
    "        \n",
    "        if self.training and self.mixup_enabled and targets is not None:\n",
    "            loss = self.mixup_criterion(F.binary_cross_entropy_with_logits, \n",
    "                                       logits, targets_a, targets_b, lam)\n",
    "            return logits, loss\n",
    "            \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "351ad556",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.425051Z",
     "iopub.status.busy": "2025-04-17T07:51:08.424831Z",
     "iopub.status.idle": "2025-04-17T07:51:08.430105Z",
     "shell.execute_reply": "2025-04-17T07:51:08.429440Z"
    },
    "papermill": {
     "duration": 0.009711,
     "end_time": "2025-04-17T07:51:08.431189",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.421478",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Estimate Class Occurrence Frequency and Set Dynamic Thresholds\n",
    "def estimate_class_thresholds():\n",
    "    \"\"\"\n",
    "    Estimate optimal prediction thresholds for each bird species class,\n",
    "    based on taxonomy and observed frequency.\n",
    "\n",
    "    Rare species usually require higher thresholds to reduce false positives,\n",
    "    while more common species may allow for lower thresholds to ensure recall.\n",
    "\n",
    "    Returns:\n",
    "        thresholds (dict): A dictionary mapping each species to its threshold.\n",
    "    \"\"\"\n",
    "    print(\"Estimating optimal thresholds for each class...\")\n",
    "\n",
    "    # Default threshold defined in config\n",
    "    default_threshold = CFG.threshold\n",
    "    \n",
    "    # Dictionary to hold species-specific thresholds\n",
    "    thresholds = {}\n",
    "\n",
    "    # If taxonomy data is available and contains valid species labels\n",
    "    if taxonomy is not None and 'primary_label' in taxonomy.columns:\n",
    "        for species in primary_labels:\n",
    "            species_info = taxonomy[taxonomy['primary_label'] == species]\n",
    "\n",
    "            if not species_info.empty:\n",
    "                # Extract taxonomic details\n",
    "                family = species_info['family'].iloc[0] if 'family' in species_info.columns else None\n",
    "                genus = species_info['genus'].iloc[0] if 'genus' in species_info.columns else None\n",
    "\n",
    "                # Example logic:\n",
    "                # Certain families are more common (e.g., hummingbirds), allow lower threshold.\n",
    "                # Others may be rare or have similar calls, requiring higher threshold.\n",
    "                if family in ['Trochilidae', 'Tyrannidae']:  # Hummingbirds, Tyrant Flycatchers\n",
    "                    thresholds[species] = default_threshold - 0.05  # Slightly lower threshold for recall\n",
    "                elif family in ['Thraupidae', 'Parulidae']:  # Tanagers, Wood-warblers\n",
    "                    thresholds[species] = default_threshold + 0.05  # Slightly higher threshold to avoid confusion\n",
    "                else:\n",
    "                    thresholds[species] = default_threshold\n",
    "            else:\n",
    "                # Species not found in taxonomy, fallback to default\n",
    "                thresholds[species] = default_threshold\n",
    "    else:\n",
    "        # If no taxonomy data is available, apply default threshold for all species\n",
    "        for species in primary_labels:\n",
    "            thresholds[species] = default_threshold\n",
    "\n",
    "    return thresholds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cdcce5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.438526Z",
     "iopub.status.busy": "2025-04-17T07:51:08.438285Z",
     "iopub.status.idle": "2025-04-17T07:51:08.443673Z",
     "shell.execute_reply": "2025-04-17T07:51:08.443119Z"
    },
    "papermill": {
     "duration": 0.010758,
     "end_time": "2025-04-17T07:51:08.444734",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.433976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Audio File Loading Utility\n",
    "def get_audio_files():\n",
    "    \"\"\"\n",
    "    Retrieves the list of audio files to process based on mode (test or debug).\n",
    "\n",
    "    If test files are found in the test_soundscapes directory, the function switches\n",
    "    to full evaluation mode. If not, it defaults to debug mode and uses a subset\n",
    "    of training soundscapes instead.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (audio_paths, file_ids)\n",
    "            - audio_paths: Full file paths to audio files.\n",
    "            - file_ids: Unique identifiers (filenames without extensions).\n",
    "    \"\"\"\n",
    "    if os.path.exists(CFG.test_soundscapes) and len(glob.glob(f'{CFG.test_soundscapes}/*.ogg')) > 0:\n",
    "        # Found official test soundscapes, switch to full inference mode\n",
    "        CFG.debug = False\n",
    "        audio_dir = CFG.test_soundscapes\n",
    "        audio_paths = sorted(glob.glob(f'{audio_dir}/*.ogg'))\n",
    "    else:\n",
    "        # No test files found — fallback to debug mode using train soundscapes\n",
    "        print(\"No test files found. Using train soundscapes for debugging.\")\n",
    "        CFG.debug = True\n",
    "        audio_dir = CFG.train_soundscapes\n",
    "        all_audio_paths = sorted(glob.glob(f'{audio_dir}/*.ogg'))\n",
    "\n",
    "        # Only take a limited subset of audio files for quick debugging\n",
    "        audio_paths = all_audio_paths[CFG.debug_start_num:CFG.debug_start_num + CFG.debug_num]\n",
    "\n",
    "    # Extract file IDs from file paths (remove directory and extension)\n",
    "    file_ids = [os.path.splitext(os.path.basename(path))[0] for path in audio_paths]\n",
    "\n",
    "    # Summary output\n",
    "    print(f'Debug mode: {CFG.debug}')\n",
    "    print(f'Number of soundscapes: {len(audio_paths)}')\n",
    "\n",
    "    return audio_paths, file_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b908e57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.451159Z",
     "iopub.status.busy": "2025-04-17T07:51:08.450919Z",
     "iopub.status.idle": "2025-04-17T07:51:08.454468Z",
     "shell.execute_reply": "2025-04-17T07:51:08.453968Z"
    },
    "papermill": {
     "duration": 0.007741,
     "end_time": "2025-04-17T07:51:08.455441",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.447700",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Model Discovery and Loading\n",
    "def find_model_files():\n",
    "    \"\"\"\n",
    "    Finds and loads all .pth model files in the specified model directory.\n",
    "\n",
    "    This function searches recursively inside the directory defined by `CFG.model_path`,\n",
    "    collecting all PyTorch model checkpoint files (with .pth extension).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of full paths to discovered model files.\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    model_dir = Path(CFG.model_path)\n",
    "    \n",
    "    # Recursively search for all .pth files in the model directory\n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7be3ebfb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.461899Z",
     "iopub.status.busy": "2025-04-17T07:51:08.461707Z",
     "iopub.status.idle": "2025-04-17T07:51:08.466917Z",
     "shell.execute_reply": "2025-04-17T07:51:08.466428Z"
    },
    "papermill": {
     "duration": 0.009719,
     "end_time": "2025-04-17T07:51:08.467965",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.458246",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_models():\n",
    "    \"\"\"\n",
    "    Loads all discovered model files and prepares them for ensemble inference.\n",
    "    \n",
    "    This function:\n",
    "    - Searches for all .pth model files under the configured model directory\n",
    "    - Optionally filters by specified folds if CFG.use_specific_folds is True\n",
    "    - Loads the model state dictionaries\n",
    "    - Initializes BirdCLEFModel instances with loaded weights\n",
    "    - Moves models to the appropriate device (CPU/GPU) and sets them to eval mode\n",
    "    \n",
    "    Returns:\n",
    "        list: A list of loaded PyTorch model instances ready for inference.\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    model_files = find_model_files()\n",
    "\n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {CFG.model_path}!\")\n",
    "        return models\n",
    "\n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "\n",
    "    # Optionally filter by specific folds (e.g., only fold0 and fold1)\n",
    "    if CFG.use_specific_folds:\n",
    "        filtered_files = []\n",
    "        for fold in CFG.folds:\n",
    "            fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "            filtered_files.extend(fold_files)\n",
    "        model_files = filtered_files\n",
    "        print(f\"Using {len(model_files)} model files for the specified folds ({CFG.folds}).\")\n",
    "    \n",
    "    # Load each model from its checkpoint\n",
    "    for model_path in model_files:\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location='cpu')  # Always load on CPU first\n",
    "            model = BirdCLEFModel(CFG)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(CFG.device)  # Move to configured device\n",
    "            model.eval()  # Set to evaluation mode\n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43dda06c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.474506Z",
     "iopub.status.busy": "2025-04-17T07:51:08.474328Z",
     "iopub.status.idle": "2025-04-17T07:51:08.499460Z",
     "shell.execute_reply": "2025-04-17T07:51:08.498958Z"
    },
    "papermill": {
     "duration": 0.029681,
     "end_time": "2025-04-17T07:51:08.500470",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.470789",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Audio Processing Functions\n",
    "\n",
    "def reduce_noise(audio_data):\n",
    "    \"\"\"\n",
    "    Apply noise reduction to raw audio using median filtering and signal blending.\n",
    "\n",
    "    Args:\n",
    "        audio_data (np.ndarray): Input 1D audio waveform.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Denoised audio signal.\n",
    "    \"\"\"\n",
    "    if not CFG.apply_noise_reduction:\n",
    "        return audio_data\n",
    "\n",
    "    # Apply median filter for noise suppression\n",
    "    window_size = 5\n",
    "    audio_denoised = signal.medfilt(audio_data, window_size)\n",
    "\n",
    "    # Blend original signal with denoised version\n",
    "    return (1 - CFG.noise_reduction_strength) * audio_data + CFG.noise_reduction_strength * audio_denoised\n",
    "\n",
    "\n",
    "def normalize_audio(audio_data):\n",
    "    \"\"\"\n",
    "    Normalize the audio waveform by removing DC offset and scaling amplitude.\n",
    "\n",
    "    Args:\n",
    "        audio_data (np.ndarray): Input 1D audio waveform.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Normalized audio signal.\n",
    "    \"\"\"\n",
    "    if not CFG.apply_normalization:\n",
    "        return audio_data\n",
    "\n",
    "    # Remove DC offset\n",
    "    audio_data = audio_data - np.mean(audio_data)\n",
    "\n",
    "    # Normalize amplitude to [-1, 1]\n",
    "    max_amplitude = np.max(np.abs(audio_data))\n",
    "    if max_amplitude > 0:\n",
    "        audio_data = audio_data / max_amplitude\n",
    "\n",
    "    return audio_data\n",
    "\n",
    "\n",
    "def audio2melspec(audio_data, cfg=CFG):\n",
    "    \"\"\"\n",
    "    Convert raw audio to a normalized Mel-spectrogram with optional enhancements.\n",
    "\n",
    "    This includes:\n",
    "    - Noise reduction\n",
    "    - Normalization\n",
    "    - Spectrogram contrast enhancement\n",
    "    - Resizing to target shape\n",
    "\n",
    "    Args:\n",
    "        audio_data (np.ndarray): Raw audio waveform.\n",
    "        cfg (CFG): Configuration object containing processing parameters.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed Mel-spectrogram (float32).\n",
    "    \"\"\"\n",
    "    # Replace NaNs with mean value (safety check based on reference code)\n",
    "    if np.isnan(audio_data).any():\n",
    "        mean_signal = np.nanmean(audio_data)\n",
    "        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "    # Pad short audio to required window size\n",
    "    required_length = cfg.FS * cfg.WINDOW_SIZE\n",
    "    if len(audio_data) < required_length:\n",
    "        audio_data = np.pad(\n",
    "            audio_data,\n",
    "            (0, required_length - len(audio_data)),\n",
    "            mode='constant'\n",
    "        )\n",
    "\n",
    "    # Apply noise reduction and normalization\n",
    "    audio_data = reduce_noise(audio_data)\n",
    "    audio_data = normalize_audio(audio_data)\n",
    "\n",
    "    # Compute Mel-spectrogram\n",
    "    mel_spec = librosa.feature.melspectrogram(\n",
    "        y=audio_data,\n",
    "        sr=cfg.FS,\n",
    "        n_fft=cfg.N_FFT,\n",
    "        hop_length=cfg.HOP_LENGTH,\n",
    "        n_mels=cfg.N_MELS,\n",
    "        fmin=cfg.FMIN,\n",
    "        fmax=cfg.FMAX,\n",
    "        power=2.0  # Use power spectrogram\n",
    "    )\n",
    "\n",
    "    # Convert to log scale (dB)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Normalize to [0, 1]\n",
    "    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "\n",
    "    # Optional: Apply contrast enhancement to highlight features\n",
    "    if cfg.apply_spec_contrast:\n",
    "        mel_spec_norm = enhance_spectrogram_contrast(mel_spec_norm, cfg.contrast_factor)\n",
    "\n",
    "    # Resize to fixed target shape (e.g., 256x256)\n",
    "    if mel_spec_norm.shape != cfg.TARGET_SHAPE:\n",
    "        mel_spec_norm = cv2.resize(mel_spec_norm, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return mel_spec_norm.astype(np.float32)\n",
    "\n",
    "# The `factor` parameter controls the strength of contrast enhancement; try values between 0.05 and 0.2\n",
    "def enhance_spectrogram_contrast(spec, factor=0.15):\n",
    "    \"\"\"\n",
    "    Enhance the contrast of a mel-spectrogram to make features more distinguishable.\n",
    "    \n",
    "    :param spec: Input spectrogram (2D array).\n",
    "    :param factor: Contrast enhancement factor.\n",
    "    :return: Contrast-enhanced spectrogram, clipped to [0, 1].\n",
    "    \"\"\"\n",
    "    mean = np.mean(spec)\n",
    "    enhanced = mean + (spec - mean) * (1 + factor)\n",
    "    return np.clip(enhanced, 0, 1)\n",
    "\n",
    "def apply_spec_augment(spec):\n",
    "    \"\"\"\n",
    "    Apply SpecAugment to a mel-spectrogram by adding time and frequency masks.\n",
    "    \n",
    "    :param spec: Input mel-spectrogram of shape [frequency, time].\n",
    "    :return: Augmented spectrogram.\n",
    "    \"\"\"\n",
    "    if not CFG.use_spec_augment:\n",
    "        return spec\n",
    "    \n",
    "    augmented = spec.copy()\n",
    "    \n",
    "    # Apply frequency masking\n",
    "    for _ in range(CFG.freq_mask_count):\n",
    "        f = np.random.randint(0, CFG.freq_mask_param)\n",
    "        f0 = np.random.randint(0, augmented.shape[0] - f)\n",
    "        augmented[f0:f0 + f, :] = 0  # Zero out selected frequency range\n",
    "    \n",
    "    # Apply time masking\n",
    "    for _ in range(CFG.time_mask_count):\n",
    "        t = np.random.randint(0, CFG.time_mask_param)\n",
    "        t0 = np.random.randint(0, augmented.shape[1] - t)\n",
    "        augmented[:, t0:t0 + t] = 0  # Zero out selected time range\n",
    "    \n",
    "    return augmented\n",
    "\n",
    "def apply_tta(spec, tta_idx):\n",
    "    \"\"\"\n",
    "    Apply Test-Time Augmentation (TTA) to a mel-spectrogram.\n",
    "    \n",
    "    :param spec: Input mel-spectrogram.\n",
    "    :param tta_idx: Index indicating which TTA method to apply.\n",
    "    :return: Augmented spectrogram.\n",
    "    \"\"\"\n",
    "    result = spec.copy()\n",
    "    \n",
    "    if tta_idx == 0:\n",
    "        # No augmentation\n",
    "        return result\n",
    "    elif tta_idx == 1:\n",
    "        # Horizontal flip (time reversal)\n",
    "        return np.flip(result, axis=1).copy()\n",
    "    elif tta_idx == 2:\n",
    "        # Vertical flip (frequency inversion)\n",
    "        return np.flip(result, axis=0).copy()\n",
    "    elif tta_idx == 3:\n",
    "        # Both horizontal and vertical flips\n",
    "        return np.flip(np.flip(result, axis=1), axis=0).copy()\n",
    "    elif tta_idx == 4:\n",
    "        # Slight upward pitch shift (frequency axis roll)\n",
    "        return np.roll(result, shift=3, axis=0)\n",
    "    elif tta_idx == 5:\n",
    "        # Slight downward pitch shift\n",
    "        return np.roll(result, shift=-3, axis=0)\n",
    "    else:\n",
    "        return result\n",
    "\n",
    "##############################################################################\n",
    "# Main inference function with enhancement and ensemble support\n",
    "def predict_on_audio(audio_path, models):\n",
    "    \"\"\"\n",
    "    Perform inference on a single audio file. The audio is split into 5-second segments,\n",
    "    and the presence of bird species is predicted for each segment.\n",
    "    \n",
    "    :param audio_path: Path to the audio file.\n",
    "    :param models: List of PyTorch models used for ensemble prediction.\n",
    "    :return: Tuple (row_ids, predictions) for each segment.\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = os.path.splitext(os.path.basename(audio_path))[0]\n",
    "    \n",
    "    try:\n",
    "        print(f\"Processing {soundscape_id}\")\n",
    "        audio_data, _ = librosa.load(audio_path, sr=CFG.FS)\n",
    "        total_segments = int(len(audio_data) / (CFG.FS * CFG.WINDOW_SIZE))\n",
    "        \n",
    "        segment_predictions = []  # Store predictions for each segment before post-processing\n",
    "        \n",
    "        for segment_idx in range(total_segments):\n",
    "            if time.time() > TERMINATE_TIME:\n",
    "                print(\"Time limit reached, stopping processing early\")\n",
    "                return row_ids, predictions\n",
    "                \n",
    "            start_sample = segment_idx * CFG.FS * CFG.WINDOW_SIZE\n",
    "            end_sample = start_sample + CFG.FS * CFG.WINDOW_SIZE\n",
    "            segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "            end_time_sec = (segment_idx + 1) * CFG.WINDOW_SIZE\n",
    "            row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "            row_ids.append(row_id)\n",
    "\n",
    "            try:\n",
    "                # Apply TTA if enabled\n",
    "                if CFG.use_tta:\n",
    "                    all_preds = []\n",
    "                    for tta_idx in range(CFG.tta_count):\n",
    "                        mel_spec = audio2melspec(segment_audio)\n",
    "                        \n",
    "                        # Optionally apply SpecAugment during inference\n",
    "                        if np.random.random() < 0.5 and CFG.use_spec_augment:\n",
    "                            mel_spec = apply_spec_augment(mel_spec)\n",
    "                        \n",
    "                        mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "                        mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                        mel_spec_tensor = mel_spec_tensor.to(CFG.device)\n",
    "                        \n",
    "                        segment_preds = []\n",
    "                        for model in models:\n",
    "                            with torch.no_grad():\n",
    "                                outputs = model(mel_spec_tensor)\n",
    "                                probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                                segment_preds.append(probs)\n",
    "                        \n",
    "                        # Ensemble prediction via weighted average\n",
    "                        if len(models) > 1:\n",
    "                            weights = np.linspace(0.8, 1.2, len(models))\n",
    "                            weights = weights / weights.sum()\n",
    "                            avg_preds = np.average(segment_preds, axis=0, weights=weights)\n",
    "                        else:\n",
    "                            avg_preds = segment_preds[0]\n",
    "                        \n",
    "                        all_preds.append(avg_preds)\n",
    "                    \n",
    "                    # Average predictions across all TTA outputs\n",
    "                    final_preds = np.mean(all_preds, axis=0)\n",
    "                else:\n",
    "                    # Regular inference without TTA\n",
    "                    mel_spec = audio2melspec(segment_audio)\n",
    "                    mel_spec_tensor = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "                    mel_spec_tensor = mel_spec_tensor.to(CFG.device)\n",
    "                    \n",
    "                    segment_preds = []\n",
    "                    for model in models:\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model(mel_spec_tensor)\n",
    "                            probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                            segment_preds.append(probs)\n",
    "                    \n",
    "                    if len(models) > 1:\n",
    "                        weights = np.linspace(0.8, 1.2, len(models))\n",
    "                        weights = weights / weights.sum()\n",
    "                        final_preds = np.average(segment_preds, axis=0, weights=weights)\n",
    "                    else:\n",
    "                        final_preds = segment_preds[0]\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing segment {segment_idx}: {e}\")\n",
    "                # Fallback: use zeros or previous prediction\n",
    "                final_preds = np.zeros(len(primary_labels)) if len(predictions) == 0 else predictions[-1]\n",
    "            \n",
    "            segment_predictions.append(final_preds)\n",
    "        \n",
    "        # Post-process predictions by smoothing with adjacent segments\n",
    "        for i in range(len(segment_predictions)):\n",
    "            if i > 0 and i < len(segment_predictions) - 1:\n",
    "                # Smooth current prediction using adjacent segments\n",
    "                blended_pred = (\n",
    "                    CFG.prediction_blend[0] * segment_predictions[i] + \n",
    "                    CFG.prediction_blend[1] * 0.5 * (segment_predictions[i-1] + segment_predictions[i+1])\n",
    "                )\n",
    "                predictions.append(blended_pred)\n",
    "            else:\n",
    "                # Use raw predictions for first and last segments\n",
    "                predictions.append(segment_predictions[i])\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {audio_path}: {e}\")\n",
    "        row_ids = []\n",
    "        predictions = []\n",
    "    \n",
    "    return row_ids, predictions\n",
    "\n",
    "##############################################################################\n",
    "# Enhanced smoothing function\n",
    "def smooth_predictions(submission_df):\n",
    "    \"\"\"\n",
    "    Smooth predictions in the submission to enhance temporal consistency.\n",
    "    \n",
    "    :param submission_df: DataFrame containing raw predictions.\n",
    "    :return: Smoothed predictions DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Smoothing prediction results...\")\n",
    "    sub = submission_df.copy()\n",
    "    cols = sub.columns[1:]\n",
    "    sub['group'] = sub['row_id'].str.rsplit('_', n=1).str[0]  # Extract group ID by removing timestamp\n",
    "    unique_groups = sub['group'].unique()\n",
    "    \n",
    "    for group in unique_groups:\n",
    "        group_mask = sub['group'] == group\n",
    "        sub_group = sub[group_mask].copy()\n",
    "        predictions = sub_group[cols].values\n",
    "        new_predictions = predictions.copy()\n",
    "        \n",
    "        if predictions.shape[0] > 1:\n",
    "            # Enhanced smoothing with configurable window and weights\n",
    "            window = CFG.smoothing_window\n",
    "            weights = CFG.smoothing_weights\n",
    "            half_window = window // 2\n",
    "            \n",
    "            # Handle edge cases\n",
    "            for i in range(half_window):\n",
    "                # Smooth the beginning predictions using a truncated window\n",
    "                valid_window = i + half_window + 1\n",
    "                valid_weights = weights[-valid_window:]\n",
    "                valid_weights = valid_weights / np.sum(valid_weights)\n",
    "                new_predictions[i] = np.average(predictions[:valid_window], axis=0, weights=valid_weights)\n",
    "                \n",
    "                # Smooth the end predictions similarly\n",
    "                valid_window = i + half_window + 1\n",
    "                valid_weights = weights[:valid_window]\n",
    "                valid_weights = valid_weights / np.sum(valid_weights)\n",
    "                new_predictions[-(i+1)] = np.average(predictions[-valid_window:], axis=0, weights=valid_weights)\n",
    "            \n",
    "            # Apply smoothing for central elements using sliding window\n",
    "            for i in range(half_window, predictions.shape[0] - half_window):\n",
    "                window_start = i - half_window\n",
    "                window_end = i + half_window + 1\n",
    "                new_predictions[i] = np.average(predictions[window_start:window_end], axis=0, weights=weights)\n",
    "        \n",
    "        sub.loc[group_mask, cols] = new_predictions\n",
    "\n",
    "    sub.drop('group', axis=1, inplace=True)\n",
    "    return sub\n",
    "\n",
    "##############################################################################\n",
    "# Apply thresholds to predictions\n",
    "def apply_thresholds(submission_df, thresholds):\n",
    "    \"\"\"\n",
    "    Binarize prediction results using the best threshold for each class.\n",
    "    \n",
    "    :param submission_df: DataFrame containing prediction probabilities.\n",
    "    :param thresholds: Dictionary of optimal thresholds per class.\n",
    "    :return: Thresholded binary prediction DataFrame.\n",
    "    \"\"\"\n",
    "    print(\"Applying class-specific thresholds...\")\n",
    "    result_df = submission_df.copy()\n",
    "    \n",
    "    for species in primary_labels:\n",
    "        threshold = thresholds.get(species, CFG.threshold)\n",
    "        result_df[species] = (result_df[species] >= threshold).astype(float)\n",
    "    \n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439ec285",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.506914Z",
     "iopub.status.busy": "2025-04-17T07:51:08.506738Z",
     "iopub.status.idle": "2025-04-17T07:51:08.522702Z",
     "shell.execute_reply": "2025-04-17T07:51:08.522065Z"
    },
    "papermill": {
     "duration": 0.020475,
     "end_time": "2025-04-17T07:51:08.523711",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.503236",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##############################################################################\n",
    "# Main Inference Pipeline\n",
    "def run_pipeline():\n",
    "    \"\"\"\n",
    "    Execute the complete inference pipeline.\n",
    "    \"\"\"\n",
    "    print(f\"Device: {CFG.device}\")\n",
    "    print(f\"TTA enabled: {CFG.use_tta} (variations: {CFG.tta_count if CFG.use_tta else 0})\")\n",
    "    \n",
    "    # Estimate optimal thresholds for each class (if not already set)\n",
    "    if CFG.class_thresholds is None:\n",
    "        CFG.class_thresholds = estimate_class_thresholds()\n",
    "    \n",
    "    # Load model(s)\n",
    "    models = load_models()\n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "    \n",
    "    # Retrieve all audio files\n",
    "    audio_paths, file_ids = get_audio_files()\n",
    "    print(f\"Found {len(audio_paths)} audio files\")\n",
    "    \n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "    \n",
    "    # Process each audio file individually\n",
    "    for i, audio_path in enumerate(tqdm(audio_paths)):\n",
    "        row_ids, predictions = predict_on_audio(audio_path, models)\n",
    "        \n",
    "        # Add results only if both are valid and lengths match\n",
    "        if len(row_ids) > 0 and len(predictions) > 0 and len(row_ids) == len(predictions):\n",
    "            all_row_ids.extend(row_ids)\n",
    "            all_predictions.extend(predictions)\n",
    "        else:\n",
    "            print(f\"Skipping results for {audio_path} due to length mismatch or empty results\")\n",
    "        \n",
    "        # Periodically clear memory to maintain efficiency\n",
    "        if (i + 1) % CFG.clear_cache_frequency == 0:\n",
    "            clear_memory()\n",
    "    \n",
    "    # Construct the submission DataFrame\n",
    "    print(\"Creating submission dataframe...\")\n",
    "    submission_dict = {'row_id': all_row_ids}\n",
    "    for i, species in enumerate(primary_labels):\n",
    "        submission_dict[species] = [pred[i] for pred in all_predictions]\n",
    "    \n",
    "    # Ensure consistent length across all columns\n",
    "    lengths = [len(v) for v in submission_dict.values()]\n",
    "    if len(set(lengths)) > 1:\n",
    "        print(f\"Warning: Inconsistent lengths in submission_dict: {lengths}\")\n",
    "        min_length = min(lengths)\n",
    "        for k in submission_dict:\n",
    "            submission_dict[k] = submission_dict[k][:min_length]\n",
    "    \n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "    \n",
    "    # Ensure all required columns are present (match sample submission)\n",
    "    sample_sub = pd.read_csv(CFG.submission_csv)\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "            \n",
    "    # Reorder columns to match sample submission\n",
    "    if 'row_id' in sample_sub.columns:\n",
    "        submission_df = submission_df[sample_sub.columns]\n",
    "    \n",
    "    # Apply temporal smoothing (first stage)\n",
    "    if CFG.apply_smoothing:\n",
    "        submission_df = smooth_predictions(submission_df)\n",
    "    \n",
    "    # Optional second-stage smoothing from reference implementation\n",
    "    if CFG.apply_secondary_smoothing:\n",
    "        cols = submission_df.columns[1:]\n",
    "        groups = submission_df['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "            \n",
    "        for group in np.unique(groups):\n",
    "            group_mask = (groups == group)\n",
    "            sub_group = submission_df[group_mask]\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            # Smooth internal entries using a 3-frame weighted average\n",
    "            for i in range(1, predictions.shape[0]-1):\n",
    "                new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "            \n",
    "            # Handle the first and last entries\n",
    "            if predictions.shape[0] > 1:\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "            \n",
    "            submission_df.loc[group_mask, cols] = new_predictions\n",
    "    \n",
    "    # Apply thresholds to convert probabilities to binary labels (optional)\n",
    "    # Note: This is typically skipped for Kaggle since evaluation is on probabilities\n",
    "    # final_df = apply_thresholds(submission_df, CFG.class_thresholds)\n",
    "    final_df = submission_df\n",
    "    \n",
    "    # Save to CSV\n",
    "    submission_path = 'submission.csv'\n",
    "    final_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "    # Display preview of submission\n",
    "    print(\"\\nSubmission head:\")\n",
    "    print(final_df.head(5))\n",
    "    \n",
    "    print(\"\\nSubmission tail:\")\n",
    "    print(final_df.tail(5))\n",
    "    \n",
    "    return final_df\n",
    "##############################################################################\n",
    "# Visualization Functions\n",
    "\n",
    "def create_mel_transform():\n",
    "    \"\"\"\n",
    "    Create a MelSpectrogram transform for visualization.\n",
    "    \n",
    "    :return: MelSpectrogram transform object.\n",
    "    \"\"\"\n",
    "    return AT.MelSpectrogram(\n",
    "        sample_rate=CFG.FS,\n",
    "        n_fft=CFG.N_FFT,\n",
    "        win_length=CFG.N_FFT,\n",
    "        hop_length=CFG.HOP_LENGTH,\n",
    "        center=True,\n",
    "        f_min=CFG.FMIN,\n",
    "        f_max=CFG.FMAX,\n",
    "        pad_mode=\"reflect\",\n",
    "        power=2.0,\n",
    "        norm='slaney',\n",
    "        n_mels=CFG.N_MELS,\n",
    "        mel_scale=\"htk\",\n",
    "    )\n",
    "\n",
    "def audio_to_mel_debug(filepath):\n",
    "    \"\"\"\n",
    "    Convert an audio file to its Mel spectrogram representation for debugging or visualization.\n",
    "    \n",
    "    :param filepath: Path to the audio file (.ogg, .wav, etc.)\n",
    "    :return: Tensor representing the log-scaled Mel spectrogram.\n",
    "    \"\"\"\n",
    "    waveform, _ = torchaudio.load(filepath, backend=\"soundfile\")\n",
    "    \n",
    "    # Normalize waveform to [-1, 1]\n",
    "    waveform = waveform / torch.max(torch.abs(waveform))\n",
    "    \n",
    "    # Generate mel spectrogram\n",
    "    mel_transform = create_mel_transform()\n",
    "    melspec = mel_transform(waveform)\n",
    "    \n",
    "    # Convert power to decibels (log scale), add epsilon to avoid log(0)\n",
    "    melspec = 10 * torch.log10(melspec + 1e-10)\n",
    "    \n",
    "    return melspec\n",
    "\n",
    "def plot_results(results, file_name, audio_dir):\n",
    "    \"\"\"\n",
    "    Plot the Mel spectrogram and prediction heatmap for a given audio file.\n",
    "    \n",
    "    :param results: DataFrame containing prediction results for all files.\n",
    "    :param file_name: Base name of the audio file (without extension).\n",
    "    :param audio_dir: Directory containing the audio files.\n",
    "    \"\"\"\n",
    "    path = os.path.join(audio_dir, file_name + \".ogg\")\n",
    "    \n",
    "    # Compute Mel spectrogram\n",
    "    specgram = audio_to_mel_debug(path)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "    \n",
    "    # Plot the Mel spectrogram\n",
    "    axes[0].set_title(file_name)\n",
    "    im = axes[0].imshow(specgram[0], origin=\"lower\", aspect=\"auto\")\n",
    "    axes[0].set_ylabel(\"Mel bin\")\n",
    "    axes[0].set_xlabel(\"Frame\")\n",
    "    fig.colorbar(im, ax=axes[0])\n",
    "    \n",
    "    # Filter prediction results for the current audio file\n",
    "    file_results = results[results[\"row_id\"].str.contains(file_name)]\n",
    "    \n",
    "    # Determine number of time segments (usually 60s audio → 12 segments)\n",
    "    time_segments = file_results.shape[0]\n",
    "    \n",
    "    # Plot prediction heatmap\n",
    "    heatmap = axes[1].pcolor(file_results.iloc[:, 1:].values.T, edgecolors='k',\n",
    "                             linewidths=0.1, vmin=0, vmax=1, cmap='Blues')\n",
    "    fig.colorbar(heatmap, ax=axes[1])\n",
    "    \n",
    "    # Set time ticks on x-axis (e.g., 0s, 5s, ..., 55s)\n",
    "    axes[1].set_xticks(np.arange(0, time_segments, 1))\n",
    "    axes[1].set_xticklabels(np.arange(0, time_segments * 5, 5))\n",
    "    \n",
    "    axes[1].set_ylabel(\"Species\")\n",
    "    axes[1].set_xlabel(\"Seconds\")\n",
    "    \n",
    "    # Show species labels only when count is manageable\n",
    "    if len(primary_labels) <= 30:\n",
    "        axes[1].set_yticks(np.arange(0.5, len(primary_labels), 1))\n",
    "        axes[1].set_yticklabels(primary_labels)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.savefig(f'{file_name}_prediction.png')\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d39caf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-17T07:51:08.529847Z",
     "iopub.status.busy": "2025-04-17T07:51:08.529660Z",
     "iopub.status.idle": "2025-04-17T07:51:48.909782Z",
     "shell.execute_reply": "2025-04-17T07:51:48.908913Z"
    },
    "papermill": {
     "duration": 40.384616,
     "end_time": "2025-04-17T07:51:48.911066",
     "exception": false,
     "start_time": "2025-04-17T07:51:08.526450",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "TTA enabled: False (variations: 0)\n",
      "Estimating optimal thresholds for each class...\n",
      "Found a total of 1 model files.\n",
      "Loading model: /kaggle/input/bestmodel/pytorch/default/1/model_fold4_best_20250416_214622.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28802c7fc3c345f7b5e815b6f3bf1650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/21.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model usage: Single model\n",
      "No test files found. Using train soundscapes for debugging.\n",
      "Debug mode: True\n",
      "Number of soundscapes: 8\n",
      "Found 8 audio files\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d278cd7093ef45a4ab8917eb71f0d91f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing H02_20230420_074000\n",
      "Processing H02_20230420_112000\n",
      "Processing H02_20230420_154500\n",
      "Processing H02_20230420_164000\n",
      "Processing H02_20230420_223500\n",
      "Processing H02_20230421_093000\n",
      "Processing H02_20230421_113500\n",
      "Processing H02_20230421_170000\n",
      "Creating submission dataframe...\n",
      "Smoothing prediction results...\n",
      "Submission saved to submission.csv\n",
      "\n",
      "Submission head:\n",
      "                   row_id   1139490   1192948   1194042    126247   1346504  \\\n",
      "0   H02_20230420_074000_5  0.000092  0.000076  0.000376  0.000064  0.000494   \n",
      "1  H02_20230420_074000_10  0.000091  0.000080  0.000385  0.000060  0.000471   \n",
      "2  H02_20230420_074000_15  0.000089  0.000085  0.000400  0.000058  0.000440   \n",
      "3  H02_20230420_074000_20  0.000088  0.000085  0.000404  0.000057  0.000408   \n",
      "4  H02_20230420_074000_25  0.000098  0.000086  0.000427  0.000064  0.000463   \n",
      "\n",
      "     134933    135045   1462711   1462737  ...   yebfly1   yebsee1   yecspi2  \\\n",
      "0  0.004250  0.002491  0.000096  0.000053  ...  0.007989  0.005650  0.043839   \n",
      "1  0.003871  0.002831  0.000100  0.000054  ...  0.010573  0.005747  0.048717   \n",
      "2  0.003425  0.003021  0.000105  0.000056  ...  0.013382  0.005943  0.050848   \n",
      "3  0.003045  0.002838  0.000108  0.000058  ...  0.015047  0.005972  0.043354   \n",
      "4  0.003170  0.002533  0.000113  0.000066  ...  0.014584  0.006485  0.032099   \n",
      "\n",
      "    yectyr1   yehbla2   yehcar1   yelori1   yeofly1   yercac1    ywcpar  \n",
      "0  0.000933  0.002593  0.003192  0.000441  0.001766  0.000286  0.000096  \n",
      "1  0.000949  0.002926  0.003236  0.000487  0.002473  0.000348  0.000102  \n",
      "2  0.001044  0.003097  0.003083  0.000553  0.003342  0.000454  0.000112  \n",
      "3  0.001315  0.002621  0.003054  0.000622  0.004339  0.000619  0.000132  \n",
      "4  0.001821  0.001796  0.003187  0.000667  0.005000  0.000731  0.000175  \n",
      "\n",
      "[5 rows x 207 columns]\n",
      "\n",
      "Submission tail:\n",
      "                    row_id   1139490   1192948   1194042    126247   1346504  \\\n",
      "91  H02_20230421_170000_40  0.000009  0.000008  0.000013  0.000004  0.000005   \n",
      "92  H02_20230421_170000_45  0.000011  0.000006  0.000012  0.000006  0.000008   \n",
      "93  H02_20230421_170000_50  0.000015  0.000008  0.000016  0.000009  0.000012   \n",
      "94  H02_20230421_170000_55  0.000020  0.000011  0.000022  0.000011  0.000016   \n",
      "95  H02_20230421_170000_60  0.000024  0.000013  0.000027  0.000012  0.000018   \n",
      "\n",
      "      134933    135045   1462711   1462737  ...   yebfly1   yebsee1   yecspi2  \\\n",
      "91  0.000053  0.000053  0.000012  0.000008  ...  0.001584  0.000490  0.000299   \n",
      "92  0.000073  0.000035  0.000013  0.000005  ...  0.001088  0.000405  0.000287   \n",
      "93  0.000101  0.000041  0.000018  0.000005  ...  0.000851  0.000546  0.000358   \n",
      "94  0.000129  0.000054  0.000025  0.000007  ...  0.000701  0.000752  0.000396   \n",
      "95  0.000139  0.000061  0.000032  0.000008  ...  0.000693  0.000972  0.000401   \n",
      "\n",
      "     yectyr1   yehbla2   yehcar1   yelori1   yeofly1   yercac1    ywcpar  \n",
      "91  0.025879  0.000365  0.002688  0.001264  0.001113  0.005356  0.001428  \n",
      "92  0.036367  0.000219  0.004439  0.001010  0.001806  0.007432  0.002296  \n",
      "93  0.043115  0.000302  0.005877  0.000739  0.003358  0.009694  0.003025  \n",
      "94  0.041971  0.000445  0.006462  0.000532  0.005081  0.011368  0.003349  \n",
      "95  0.039974  0.000571  0.006305  0.000441  0.006792  0.013055  0.003467  \n",
      "\n",
      "[5 rows x 207 columns]\n",
      "No test files found. Using train soundscapes for debugging.\n",
      "Debug mode: True\n",
      "Number of soundscapes: 8\n",
      "\n",
      "Generating visualizations...\n",
      "Visualization saved for H02_20230420_074000\n",
      "Visualization saved for H02_20230420_112000\n",
      "Visualization saved for H02_20230420_154500\n",
      "Visualization saved for H02_20230420_164000\n",
      "Visualization saved for H02_20230420_223500\n",
      "Visualization saved for H02_20230421_093000\n",
      "Visualization saved for H02_20230421_113500\n",
      "Visualization saved for H02_20230421_170000\n",
      "\n",
      "Total execution time: 0.67 minutes\n"
     ]
    }
   ],
   "source": [
    "##############################################################################\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    START = time.time()\n",
    "    TERMINATE_TIME = START + 5300  # Execution limit: ~88 minutes\n",
    "\n",
    "    try:\n",
    "        # Run the full inference pipeline\n",
    "        results = run_pipeline()\n",
    "        \n",
    "        # Clear memory after prediction to ensure sufficient memory for visualization\n",
    "        clear_memory()\n",
    "        \n",
    "        # If in debug mode and results are available, generate visualizations\n",
    "        if CFG.debug and results is not None:\n",
    "            audio_paths, file_ids = get_audio_files()\n",
    "            audio_dir = CFG.train_soundscapes if CFG.debug else CFG.test_soundscapes\n",
    "            \n",
    "            print(\"\\nGenerating visualizations...\")\n",
    "            for file_id in file_ids:\n",
    "                plot_results(results, file_id, audio_dir)\n",
    "                print(f\"Visualization saved for {file_id}\")\n",
    "        \n",
    "        print(f\"\\nTotal execution time: {(time.time() - START) / 60:.2f} minutes\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e682ca",
   "metadata": {
    "papermill": {
     "duration": 0.003812,
     "end_time": "2025-04-17T07:51:48.919128",
     "exception": false,
     "start_time": "2025-04-17T07:51:48.915316",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 304337,
     "modelInstanceId": 283480,
     "sourceId": 339012,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 306950,
     "modelInstanceId": 286116,
     "sourceId": 342049,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31012,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 67.488672,
   "end_time": "2025-04-17T07:51:51.684338",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-17T07:50:44.195666",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00c5a6017937431c9f6130de1b941753": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "156388dbf1184365b7c5bdb79ff3530f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1bdceb7e07b7443e84ad4464411eca62": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "21e0d6af98c746e5a897f97154f7de17": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "28802c7fc3c345f7b5e815b6f3bf1650": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_7004316d34b54e71afc8f555d1510c79",
        "IPY_MODEL_a7e12f5bb85c404b81a1db2251667339",
        "IPY_MODEL_cea50237ec014aaba3567eb3851ae9b9"
       ],
       "layout": "IPY_MODEL_40c46e6ba2f646b2a658964b30965abe",
       "tabbable": null,
       "tooltip": null
      }
     },
     "395b4941810346d7bc36b9a11a19b8a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3e85768fd4054325a96e40eab7c11969": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "40c46e6ba2f646b2a658964b30965abe": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5efcbc8d455f4e2ea69bed4ab085dcfa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "63e5b417013a43dea4ba67acb680d145": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_5efcbc8d455f4e2ea69bed4ab085dcfa",
       "max": 8.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ac6aad694d22455aa87661a4d85c5684",
       "tabbable": null,
       "tooltip": null,
       "value": 8.0
      }
     },
     "7004316d34b54e71afc8f555d1510c79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_156388dbf1184365b7c5bdb79ff3530f",
       "placeholder": "​",
       "style": "IPY_MODEL_e31f7107eb8e45ada3169bb6e1cbace5",
       "tabbable": null,
       "tooltip": null,
       "value": "model.safetensors: 100%"
      }
     },
     "719d3b2f9e1e404ca49cd95425652f3e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b849900da1bc43cba79605ec3189b879",
       "placeholder": "​",
       "style": "IPY_MODEL_ea78695c4b8743c1b6ce56f0db1c5e67",
       "tabbable": null,
       "tooltip": null,
       "value": "100%"
      }
     },
     "86a9233f247b46e9a90b97d112a4c0fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a7e12f5bb85c404b81a1db2251667339": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_86a9233f247b46e9a90b97d112a4c0fa",
       "max": 21355344.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_21e0d6af98c746e5a897f97154f7de17",
       "tabbable": null,
       "tooltip": null,
       "value": 21355344.0
      }
     },
     "ac6aad694d22455aa87661a4d85c5684": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "b849900da1bc43cba79605ec3189b879": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cea50237ec014aaba3567eb3851ae9b9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1bdceb7e07b7443e84ad4464411eca62",
       "placeholder": "​",
       "style": "IPY_MODEL_395b4941810346d7bc36b9a11a19b8a6",
       "tabbable": null,
       "tooltip": null,
       "value": " 21.4M/21.4M [00:00&lt;00:00, 31.9MB/s]"
      }
     },
     "d278cd7093ef45a4ab8917eb71f0d91f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_719d3b2f9e1e404ca49cd95425652f3e",
        "IPY_MODEL_63e5b417013a43dea4ba67acb680d145",
        "IPY_MODEL_e8d29a35ee384b54a288b04f52b5d353"
       ],
       "layout": "IPY_MODEL_da42764546544faa8afa598ec531877a",
       "tabbable": null,
       "tooltip": null
      }
     },
     "da42764546544faa8afa598ec531877a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e31f7107eb8e45ada3169bb6e1cbace5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e8d29a35ee384b54a288b04f52b5d353": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_3e85768fd4054325a96e40eab7c11969",
       "placeholder": "​",
       "style": "IPY_MODEL_00c5a6017937431c9f6130de1b941753",
       "tabbable": null,
       "tooltip": null,
       "value": " 8/8 [00:22&lt;00:00,  1.42s/it]"
      }
     },
     "ea78695c4b8743c1b6ce56f0db1c5e67": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
